{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ad91570",
   "metadata": {},
   "source": [
    "## Canonical 0D data split\n",
    "\n",
    "We want a set of splits following this recipe:\n",
    "\n",
    "- First take away the last 96 records, which are meant as an external validation set (this is the last plate ice-12-103)\n",
    "- Of all remaining data (A), take away a random test set which is 10% of all data\n",
    "- Of remaining data B, take away random validation set (10% of A, 11% of B)\n",
    "\n",
    "Note:\n",
    "- We do not actually have a 0D split as we do not ensure that every reactant showing up in test has also been seen in train. We simplify things and do a random split. This should usually be very close to a 0D split\n",
    "- We use a 10-fold shuffle split, not CV. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6026db0",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-11T17:48:33.317227Z",
     "end_time": "2023-04-11T17:48:33.327634Z"
    }
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"../\"))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit\n",
    "\n",
    "from src.data import SLAPData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc19918e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-11T17:48:33.322541Z",
     "end_time": "2023-04-11T17:48:35.575088Z"
    }
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "data_path = os.path.abspath(\"../data/Data S4.csv\")\n",
    "\n",
    "data = SLAPData(data_path)\n",
    "\n",
    "data.load_data_from_file()\n",
    "data.split_reaction_smiles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4334e311",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-11T17:48:35.575949Z",
     "end_time": "2023-04-11T17:48:35.578418Z"
    }
   },
   "outputs": [],
   "source": [
    "print(data.groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff3e921",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-11T17:48:35.579597Z",
     "end_time": "2023-04-11T17:48:35.582136Z"
    }
   },
   "outputs": [],
   "source": [
    "len(data.all_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aede08d8",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-11T17:48:35.584140Z",
     "end_time": "2023-04-11T17:48:35.586161Z"
    }
   },
   "outputs": [],
   "source": [
    "splitter = ShuffleSplit(n_splits=10, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d870e818",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-11T17:48:35.590629Z",
     "end_time": "2023-04-11T17:48:35.611475Z"
    }
   },
   "outputs": [],
   "source": [
    "# we use only the first 763 records, as the validation plate starts after that\n",
    "# (can be checked in generate_ml_datasets.ipynb).\n",
    "# Note that this is only applicable to the LCMS data set, not the isolated yields, which have less entries\n",
    "\n",
    "\n",
    "train_counter, val_counter, test_counter = 0, 0, 0\n",
    "train_pos_class, val_pos_class, test_pos_class = 0, 0, 0\n",
    "\n",
    "for i, (data_subset_B, test_0D) in enumerate(splitter.split(data.all_X[:763])):\n",
    "    # we take a (0D) validation set. Rest is training set\n",
    "    train, val = train_test_split(data_subset_B, test_size=0.11, random_state=None)  # <-- I forgot to seed this rng\n",
    "    \n",
    "    # update counters\n",
    "    train_counter += len(train)\n",
    "    val_counter += len(val)\n",
    "    test_counter += len(test_0D)\n",
    "    train_pos_class += np.sum(data.all_y[train])\n",
    "    val_pos_class += np.sum(data.all_y[val])\n",
    "    test_pos_class += np.sum(data.all_y[test_0D])\n",
    "    \n",
    "    \n",
    "    print(f\"Statistics for fold {i}:\")\n",
    "    print(f\"ID \\t\\t num \\t|\\t %positive\")\n",
    "    print(f\"Train: \\t\\t {len(train)} \\t|\\t {np.mean(data.all_y[train]):.0%}\")\n",
    "    print(f\"Val: \\t\\t {len(val)} \\t|\\t {np.mean(data.all_y[val]):.0%}\")\n",
    "    print(f\"Test_0D: \\t {len(test_0D)} \\t|\\t {np.mean(data.all_y[test_0D]):.0%}\")\n",
    "    print()\n",
    "    \n",
    "    # save the indices\n",
    "    save = False\n",
    "    if save:\n",
    "        save_path = pathlib.Path(\"../data/dataset_splits/LCMS_split_763records_0Dsplit_10fold/\")\n",
    "        save_path.mkdir(parents=True, exist_ok=True)\n",
    "        pd.DataFrame(train).to_csv(save_path / f\"fold{i}_train.csv\", index=False, header=None)\n",
    "        pd.DataFrame(val).to_csv(save_path / f\"fold{i}_val.csv\", index=False, header=None)\n",
    "        pd.DataFrame(test_0D).to_csv(save_path / f\"fold{i}_test_0D.csv\", index=False, header=None)\n",
    "        \n",
    "# summary statistics\n",
    "n = train_counter + val_counter + test_counter\n",
    "print(\"\\nSummary statistics:\")\n",
    "print(f\"Split sizes: {train_counter/n:.0%} train, {val_counter/n:.0%} val, {test_counter/n:.0%} test\")\n",
    "print(f\"Class balance (positive class ratio): {train_pos_class/train_counter:.0%} train, {val_pos_class/val_counter:.0%} val, {test_pos_class/test_counter:.0%} test\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
