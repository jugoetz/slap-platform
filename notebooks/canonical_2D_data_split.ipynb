{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ad91570",
   "metadata": {},
   "source": [
    "## 2D data split\n",
    "\n",
    "(adapted from canonical_data_split.ipynb)\n",
    "We want a set of splits following this recipe:\n",
    "\n",
    "- First take away the last 96 records, which are meant as an external validation set (this is the last plate ice-12-103)\n",
    "- Of all remaining data, take away a 2D test set\n",
    "- Of remaining data A, take away 2D validation set (10% of A)\n",
    "\n",
    "The size of the 2D test set can only be chosen approximately, due to the nature of the split.  We aim for a 2D test set that is of a similar since as val.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6026db0",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-11T17:51:52.391979Z",
     "end_time": "2023-04-11T17:51:52.414300Z"
    }
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"../\"))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from src.train_test_split import GroupShuffleSplit2D\n",
    "from src.data import SLAPData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc19918e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-11T17:51:52.399350Z",
     "end_time": "2023-04-11T17:51:54.724017Z"
    }
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "data_path = os.path.abspath(\"../data/Data S4.csv\")\n",
    "\n",
    "data = SLAPData(data_path)\n",
    "\n",
    "data.load_data_from_file()\n",
    "data.split_reaction_smiles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4334e311",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-11T17:51:54.724636Z",
     "end_time": "2023-04-11T17:51:54.726403Z"
    }
   },
   "outputs": [],
   "source": [
    "print(data.groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff3e921",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-11T17:51:54.726564Z",
     "end_time": "2023-04-11T17:51:54.729684Z"
    }
   },
   "outputs": [],
   "source": [
    "len(data.all_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aede08d8",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-11T17:51:54.731617Z",
     "end_time": "2023-04-11T17:51:54.733999Z"
    }
   },
   "outputs": [],
   "source": [
    "splitter = GroupShuffleSplit2D(n_splits=5, test_size=10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d870e818",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-11T17:51:54.737881Z",
     "end_time": "2023-04-11T17:51:54.765616Z"
    }
   },
   "outputs": [],
   "source": [
    "# we use only the first 763 records, as the validation plate starts after that\n",
    "# (can be checked in generate_ml_datasets.ipynb).\n",
    "# Note that this is only applicable to the LCMS data set, not the isolated yields, which have less entries\n",
    "train_counter, val_counter, test_counter = 0, 0, 0\n",
    "train_pos_class, val_pos_class, test_pos_class = 0, 0, 0\n",
    "for i, (data_subset_A, test_2D) in enumerate(splitter.split(data.all_X[:763], groups=data.groups[:763])):\n",
    "    \n",
    "    # we take a (2D) validation set. Rest is training set\n",
    "    inner_splitter = GroupShuffleSplit2D(n_splits=1, test_size=11, random_state=123)\n",
    "    train_idx, val_2D_idx = next(inner_splitter.split(data_subset_A, groups=data.groups[data_subset_A]))\n",
    "    train = data_subset_A[train_idx]    # it may be slightly confusing that we index a list of indices here, \n",
    "    val_2D = data_subset_A[val_2D_idx]  # but it is necessary, as the splitter returns indices for data_subset_A.\n",
    "    \n",
    "    # update counters\n",
    "    train_counter += len(train)\n",
    "    val_counter += len(val_2D)\n",
    "    test_counter += len(test_2D)\n",
    "    train_pos_class += np.sum(data.all_y[train])\n",
    "    val_pos_class += np.sum(data.all_y[val_2D])\n",
    "    test_pos_class += np.sum(data.all_y[test_2D])\n",
    "    \n",
    "    print(f\"Statistics for fold {i}:\")\n",
    "    print(f\"ID \\t\\t num \\t|\\t %positive\")\n",
    "    print(f\"Train: \\t\\t {len(train)} \\t|\\t {np.mean(data.all_y[train]):.0%}\")\n",
    "    print(f\"Val_2D: \\t {len(val_2D)} \\t|\\t {np.mean(data.all_y[val_2D]):.0%}\")\n",
    "    print(f\"Test_2D: \\t {len(test_2D)} \\t|\\t {np.mean(data.all_y[test_2D]):.0%}\")\n",
    "    assert np.intersect1d(val_2D, test_2D).size == 0\n",
    "    assert np.intersect1d(train, test_2D).size == 0\n",
    "    assert np.intersect1d(train, val_2D).size == 0\n",
    "    \n",
    "    # save the indices\n",
    "    save = False\n",
    "    if save:\n",
    "        save_path = pathlib.Path(\"../data/dataset_splits/LCMS_split_763records_2Dsplit_v2\")\n",
    "        save_path.mkdir(parents=True, exist_ok=True)\n",
    "        pd.DataFrame(train).to_csv(save_path / f\"fold{i}_train.csv\", index=False, header=None)\n",
    "        pd.DataFrame(val_2D).to_csv(save_path / f\"fold{i}_val.csv\", index=False, header=None)\n",
    "        pd.DataFrame(test_2D).to_csv(save_path / f\"fold{i}_test_2D.csv\", index=False, header=None)\n",
    "\n",
    "# summary statistics\n",
    "n = train_counter + val_counter + test_counter\n",
    "print(\"\\nSummary statistics:\")\n",
    "print(f\"Split sizes: {train_counter/n:.0%} train, {val_counter/n:.0%} val, {test_counter/n:.0%} test\")\n",
    "print(f\"Class balance (positive class ratio): {train_pos_class/train_counter:.0%} train, {val_pos_class/val_counter:.0%} val, {test_pos_class/test_counter:.0%} test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bf7788",
   "metadata": {},
   "source": [
    "### 10-fold ShuffleSplit\n",
    "Same as above, but with 10 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de65a2ca",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-11T17:51:54.747585Z",
     "end_time": "2023-04-11T17:51:54.765801Z"
    }
   },
   "outputs": [],
   "source": [
    "splitter = GroupShuffleSplit2D(n_splits=10, test_size=10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed772cb6",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-11T17:51:54.754111Z",
     "end_time": "2023-04-11T17:51:54.773605Z"
    }
   },
   "outputs": [],
   "source": [
    "# we use only the first 763 records, as the validation plate starts after that\n",
    "# (can be checked in generate_ml_datasets.ipynb).\n",
    "# Note that this is only applicable to the LCMS data set, not the isolated yields, which have less entries\n",
    "\n",
    "train_counter, val_counter, test_counter = 0, 0, 0\n",
    "train_pos_class, val_pos_class, test_pos_class = 0, 0, 0\n",
    "\n",
    "for i, (data_subset_A, test_2D) in enumerate(splitter.split(data.all_X[:763], groups=data.groups[:763])):\n",
    "    \n",
    "    # we take a (2D) validation set. Rest is training set\n",
    "    inner_splitter = GroupShuffleSplit2D(n_splits=1, test_size=11, random_state=123)\n",
    "    train_idx, val_2D_idx = next(inner_splitter.split(data_subset_A, groups=data.groups[data_subset_A]))\n",
    "    train= data_subset_A[train_idx]    # it may be slightly confusing that we index a list of indices here, \n",
    "    val_2D = data_subset_A[val_2D_idx]  # but it is necessary, as the splitter returns indices for data_subset_A.\n",
    "    \n",
    "    # update counters\n",
    "    train_counter += len(train)\n",
    "    val_counter += len(val_2D)\n",
    "    test_counter += len(test_2D)\n",
    "    train_pos_class += np.sum(data.all_y[train])\n",
    "    val_pos_class += np.sum(data.all_y[val_2D])\n",
    "    test_pos_class += np.sum(data.all_y[test_2D])\n",
    "    \n",
    "    print(f\"Statistics for fold {i}:\")\n",
    "    print(f\"ID \\t\\t num \\t|\\t %positive\")\n",
    "    print(f\"Train: \\t\\t {len(train)} \\t|\\t {np.mean(data.all_y[train]):.0%}\")\n",
    "    print(f\"Val_2D: \\t {len(val_2D)} \\t|\\t {np.mean(data.all_y[val_2D]):.0%}\")\n",
    "    print(f\"Test_2D: \\t {len(test_2D)} \\t|\\t {np.mean(data.all_y[test_2D]):.0%}\")\n",
    "    assert np.intersect1d(val_2D, test_2D).size == 0\n",
    "    assert np.intersect1d(train, test_2D).size == 0\n",
    "    assert np.intersect1d(train, val_2D).size == 0\n",
    "    \n",
    "    \n",
    "    # save the indices\n",
    "    save = False\n",
    "    if save:\n",
    "        save_path = pathlib.Path(\"../data/dataset_splits/LCMS_split_763records_2Dsplit_10fold_v2\")\n",
    "        save_path.mkdir(parents=True, exist_ok=True)\n",
    "        pd.DataFrame(train).to_csv(save_path / f\"fold{i}_train.csv\", index=False, header=None)\n",
    "        pd.DataFrame(val_2D).to_csv(save_path / f\"fold{i}_val.csv\", index=False, header=None)\n",
    "        pd.DataFrame(test_2D).to_csv(save_path / f\"fold{i}_test_2D.csv\", index=False, header=None)\n",
    "    \n",
    "# summary statistics\n",
    "n = train_counter + val_counter + test_counter\n",
    "print(\"\\nSummary statistics:\")\n",
    "print(f\"Split sizes: {train_counter/n:.0%} train, {val_counter/n:.0%} val, {test_counter/n:.0%} test\")\n",
    "print(f\"Class balance (positive class ratio): {train_pos_class/train_counter:.0%} train, {val_pos_class/val_counter:.0%} val, {test_pos_class/test_counter:.0%} test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f52254b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-11T17:51:54.771097Z",
     "end_time": "2023-04-11T17:51:54.773828Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
